[{"categories":["project"],"content":" 1 HDFS 的命令行使用如果没有配置 hadoop 的环境变量，则在 hadoop 的安装目录下的bin目录中执行以下命令，如已配置 hadoop 环境变量，则可在任意目录下执行 help 格式: hdfs dfs -help 操作命令 作用: 查看某一个操作命令的参数信息 ls 格式：hdfs dfs -ls URI 作用：类似于Linux的ls命令，显示文件列表 lsr 格式 : hdfs dfs -lsr URI 作用 : 在整个目录下递归执行ls, 与UNIX中的ls-R类似 mkdir 格式 ： hdfs dfs -mkdir [-p] \u003cpaths\u003e 作用 : 以\u003cpaths\u003e中的URI作为参数，创建目录。使用-p参数可以递归创建目录 put 格式 ： hdfs dfs -put \u003clocalsrc \u003e ... \u003cdst\u003e 作用 ： 将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标文件系统中（\u003cdst\u003e对应的路径）。也可以从标准输入中读取输入，写入目标文件系统中 hdfs dfs -put /rooot/bigdata.txt /dir1 moveFromLocal 格式： hdfs dfs -moveFromLocal \u003clocalsrc\u003e \u003cdst\u003e 作用: 和put命令类似，但是源文件localsrc拷贝之后自身被删除 hdfs dfs -moveFromLocal /root/bigdata.txt / copyFromLocal 格式: hdfs dfs -copyFromLocal \u003clocalsrc\u003e ... \u003cdst\u003e 作用: 从本地文件系统中拷贝文件到hdfs路径去 appendToFile 格式: hdfs dfs -appendToFile \u003clocalsrc\u003e ... \u003cdst\u003e 作用: 追加一个或者多个文件到hdfs指定文件中.也可以从命令行读取输入. hdfs dfs -appendToFile a.xml b.xml /big.xml moveToLocal 在 hadoop 2.6.4 版本测试还未未实现此方法 格式：hadoop dfs -moveToLocal [-crc] \u003csrc\u003e \u003cdst\u003e 作用：将本地文件剪切到 HDFS get 格式 hdfs dfs -get [-ignorecrc ] [-crc] \u003csrc\u003e \u003clocaldst\u003e 作用：将文件拷贝到本地文件系统。 CRC 校验失败的文件通过-ignorecrc选项拷贝。 文件和CRC校验可以通过-CRC选项拷贝 hdfs dfs -get /bigdata.txt /export/servers getmerge 格式: hdfs dfs -getmerge \u003csrc\u003e \u003clocaldst\u003e 作用: 合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,... copyToLocal 格式: hdfs dfs -copyToLocal \u003csrc\u003e ... \u003clocaldst\u003e 作用: 从hdfs拷贝到本地 mv 格式 ： hdfs dfs -mv URI \u003cdest\u003e 作用： 将hdfs上的文件从原路径移动到目标路径（移动之后文件删除），该命令不能跨文件系统 hdfs dfs -mv /dir1/bigdata.txt /dir2 rm 格式： hdfs dfs -rm [-r] 【-skipTrash】 URI 【URI 。。。】 作用： 删除参数指定的文件，参数可以有多个。 此命令只删除文件和非空目录。 如果指定-skipTrash选项，那么在回收站可用的情况下，该选项将跳过回收站而直接删除文件； 否则，在回收站可用时，在HDFS Shell 中执行此命令，会将文件暂时放到回收站中。 hdfs dfs -rm -r /dir1 cp 格式: hdfs dfs -cp URI [URI ...] \u003cdest\u003e 作用： 将文件拷贝到目标路径中。如果\u003cdest\u003e 为目录的话，可以将多个文件拷贝到该目录下。 -f 选项将覆盖目标，如果它已经存在。 -p 选项将保留文件属性（时间戳、所有权、许可、ACL、XAttr）。 hdfs dfs -cp /dir1/a.txt /dir2/bigdata.txt cat hdfs dfs -cat URI [uri ...] 作用：将参数所指示的文件内容输出到stdout hdfs dfs -cat /bigdata.txt tail 格式: hdfs dfs -tail path 作用: 显示一个文件的末尾 text 格式:hdfs dfs -text path 作用: 以字符形式打印一个文件的内容 chmod 格式:hdfs dfs -chmod [-R] URI[URI ...] 作用：改变文件权限。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 hdfs dfs -chmod -R 777 /bigdata.txt chown 格式: hdfs dfs -chmod [-R] URI[URI ...] 作用： 改变文件的所属用户和用户组。如果使用 -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。 hdfs dfs -chown -R hadoop:hadoop /bigdata.txt df 格式: hdfs dfs -df -h path 作用: 统计文件系统的可用空间信息 du 格式: hdfs dfs -du -s -h path 作用: 统计文件夹的大小信息 count 格式: hdfs dfs -count path 作用: 统计一个指定目录下的文件节点数量 setrep expunge (慎用) 格式: hdfs dfs -setrep num filePath 作用: 设置hdfs中文件的副本数量 注意: 即使设置的超过了datanode的数量,副本的数量也最多只能和datanode的数量是一致的 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:1:0","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":" 2 hdfs的高级使用命令","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:0","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":" 2.1 HDFS文件限额配置在多人共用HDFS的环境下，配置设置非常重要。特别是在 Hadoop 处理大量资料的环境，如果没有配额管理，很容易把所有的空间用完造成别人无法存取。HDFS 的配额设定是针对目录而不是针对账号，可以让每个账号仅操作某一个目录，然后对目录设置配置。 HDFS 文件的限额配置允许我们以文件个数，或者文件大小来限制我们在某个目录下上传的文件数量或者文件内容总量，以便达到我们类似百度网盘网盘等限制每个用户允许上传的最大的文件的量。 hdfs dfs -count -q -h /user/root/dir1 #查看配额信息 2.1.1 数量限额 hdfs dfs -mkdir -p /user/root/dir #创建hdfs文件夹 hdfs dfsadmin -setQuota 2 dir # 给该文件夹下面设置最多上传两个文件，发现只能上传一个文件 hdfs dfsadmin -clrQuota /user/root/dir # 清除文件数量限制 2.1.2 空间大小限额在设置空间配额时，设置的空间至少是 block_size * 3 大小 hdfs dfsadmin -setSpaceQuota 4k /user/root/dir # 限制空间大小4KB hdfs dfs -put /root/a.txt /user/root/dir # 生成任意大小的文件 dd if=/dev/zero of=1.txt bs=1M count=2 #生成2M的文件 # 清除空间配额限制 hdfs dfsadmin -clrSpaceQuota /user/root/dir ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:1","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["project"],"content":" 2.2 HDFS 的安全模式安全模式是hadoop的一种保护机制，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。 假设我们设置的副本数（即参数dfs.replication）是3，那么在datanode上就应该有3个副本存在，假设只存在2个副本，那么比例就是2/3=0.666。hdfs默认的副本率0.999。我们的副本率0.666明显小于0.999，因此系统会自动的复制副本到其他dataNode，使得副本率不小于0.999。如果系统中有5个副本，超过我们设定的3个副本，那么系统也会删除多于的2个副本。 在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在，当整个系统达到安全标准时，HDFS自动离开安全模式。30s 安全模式操作命令 hdfs dfsadmin -safemode get #查看安全模式状态 hdfs dfsadmin -safemode enter #进入安全模式 hdfs dfsadmin -safemode leave #离开安全模式 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/:2:2","tags":["Hadoop"],"title":"hadoop命令行的使用","uri":"/bigdata/hadoop/hadoop%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["开发"],"content":" HDFS HDFS概述 Hadoop 分布式系统框架中，首要的基础功能就是文件系统，在 Hadoop 中使用 FileSystem 这个抽象类来表示我们的文件系统，这个抽象类下面有很多子实现类，究竟使用哪一种，需要看我们具体的实现类，在我们实际工作中，用到的最多的就是HDFS(分布式文件系统)以及LocalFileSystem(本地文件系统)了。 在现代的企业环境中，单机容量往往无法存储大量数据，需要跨机器存储。统一管理分布在集群上的文件系统称为分布式文件系统。 HDFS（Hadoop Distributed File System）是 Hadoop 项目的一个子项目。是 Hadoop 的核心组件之一， Hadoop 非常适于存储大型数据 (比如 TB 和 PB)，其就是使用 HDFS 作为存储系统. HDFS 使用多台计算机存储文件，并且提供统一的访问接口，像是访问一个普通文件系统一样使用分布式文件系统。 HDFS架构 HDFS是一个主/从（Mater/Slave）体系结构，由三部分组成： NameNode 和 DataNode 以及 SecondaryNamenode： ● NameNode 负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。 ● DataNode 负责管理用户的文件数据块，每一个数据块都可以在多个 DataNode 上存储多个副本，默认为3个。 ● Secondary NameNode 用来监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照。最主要作用是辅助 NameNode 管理元数据信息。 HDFS的特性 首先，它是一个文件系统，用于存储文件，通过统一的命名空间目录树来定位文件； 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 master/slave 架构（主从架构） HDFS 采用 master/slave 架构。一般一个 HDFS 集群是有一个 Namenode 和一定数目的 Datanode 组成。Namenode 是 HDFS 集群主节点，Datanode 是 HDFS 集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。 分块存储 HDFS 中的文件在物理上是分块存储（block）的，块的大小可以通过配置参数来规定，默认大小在 hadoop2.x 版本中是 128M。 名字空间（NameSpace） HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。 Namenode 负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被 Namenode 记录下来。 HDFS 会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。 NameNode 元数据管理 我们把目录结构及文件分块位置信息叫做元数据。NameNode 负责维护整个 HDFS 文件系统的目录树结构，以及每一个文件所对应的 block 块信息（block 的 id，及所在的 DataNode 服务器）。 DataNode 数据存储 文件的各个 block 的具体存储管理由 DataNode 节点承担。每一个 block 都可以在多个 DataNode 上。DataNode 需要定时向 NameNode 汇报自己持有的 block 信息。 存储多个副本（副本数量也可以通过参数设置 dfs.replication，默认是 3） 副本机制 为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。 一次写入，多次读出 HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改。 正因为如此，HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做网盘等应用，因为修改不方便，延迟大，网络开销大，成本太高。 ","date":"2024-04-23","objectID":"/bigdata/hadoop/hadoop%E6%A6%82%E8%A7%88/:0:0","tags":["Hadoop"],"title":"Hadoop概览","uri":"/bigdata/hadoop/hadoop%E6%A6%82%E8%A7%88/"},{"categories":["开发"],"content":" 1 Apache Spark A Unified engine for large-scale data analytics Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing. ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:1:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":["开发"],"content":" 2 DownloadingGet Spark from the downloads page of the project website. This documentation is for Spark version 3.5.1. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI. If you’d like to build Spark from source, visit Building Spark. ","date":"2024-04-20","objectID":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/:2:0","tags":["spark"],"title":"spark简单介绍","uri":"/bigdata/spark/spark%E5%85%A5%E9%97%A8/"},{"categories":null,"content":" IntroductionThis is bold text, and this is emphasized text. Visit the Hugo website! ","date":"2024-04-20","objectID":"/posts/my-first-post/:1:0","tags":null,"title":"My First Post","uri":"/posts/my-first-post/"},{"categories":null,"content":" 常用数据库的读写方式A blog (a truncation of “weblog”) is an informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (posts). Posts are typically displayed in reverse chronological order so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual,[citation needed] occasionally of a small group, and often covered a single subject or topic. In the 2010s, “multi-author blogs” (MABs) emerged, featuring the writing of multiple authors and sometimes professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other “microblogging” systems helps integrate MABs and single-author blogs into the news media. Blog can also be used as a verb, meaning to maintain or add content to a blog. ","date":"2024-02-20","objectID":"/posts/second_post/:1:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":null,"content":" 标题二","date":"2024-02-20","objectID":"/posts/second_post/:2:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":null,"content":" 标题三","date":"2024-02-20","objectID":"/posts/second_post/:3:0","tags":"测试","title":"我的第二篇文章","uri":"/posts/second_post/"},{"categories":["开发"],"content":" 1 Pythonpython第二篇文章扽扽 ","date":"2023-02-20","objectID":"/lang/python/first/:1:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":" 2 标题二","date":"2023-02-20","objectID":"/lang/python/first/:2:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":" 3 标题三","date":"2023-02-20","objectID":"/lang/python/first/:3:0","tags":["测试"],"title":"python第二篇文章","uri":"/lang/python/first/"},{"categories":["开发"],"content":" 1 Pythonpython第一篇文章扽扽 ","date":"2023-02-20","objectID":"/lang/python/second/:1:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":" 2 标题二","date":"2023-02-20","objectID":"/lang/python/second/:2:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":" 3 标题三","date":"2023-02-20","objectID":"/lang/python/second/:3:0","tags":["测试"],"title":"python第一篇文章","uri":"/lang/python/second/"},{"categories":["开发"],"content":" 常用数据库的读写方式A blog (a truncation of “weblog”) is an informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (posts). Posts are typically displayed in reverse chronological order so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual,[citation needed] occasionally of a small group, and often covered a single subject or topic. In the 2010s, “multi-author blogs” (MABs) emerged, featuring the writing of multiple authors and sometimes professionally edited. MABs from newspapers, other media outlets, universities, think tanks, advocacy groups, and similar institutions account for an increasing quantity of blog traffic. The rise of Twitter and other “microblogging” systems helps integrate MABs and single-author blogs into the news media. Blog can also be used as a verb, meaning to maintain or add content to a blog. ","date":"2023-02-20","objectID":"/posts/first_post/:1:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"},{"categories":["开发"],"content":" 标题二","date":"2023-02-20","objectID":"/posts/first_post/:2:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"},{"categories":["开发"],"content":" 标题三","date":"2023-02-20","objectID":"/posts/first_post/:3:0","tags":["测试"],"title":"我的第一篇文章","uri":"/posts/first_post/"}]